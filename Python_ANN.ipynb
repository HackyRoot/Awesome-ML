{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python ANN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/HackyRoot/Awesome-ML/blob/master/Python_ANN.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "DNB3j6DumCO6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Network in Python"
      ]
    },
    {
      "metadata": {
        "id": "yZ29Eoahl4Un",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2 layer neural network\n",
        "\n",
        "![2 layer neural network](https://cdn-images-1.medium.com/max/800/1*sX6T0Y4aa3ARh7IBS_sdqw.png)\n"
      ]
    },
    {
      "metadata": {
        "id": "NT38UVQrkRua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, x, y):\n",
        "    self.input = x\n",
        "    self.weights1 = np.random.rand(self.input.shape[1], 4)\n",
        "    self.weights2 = np.random.rand(4, 1)\n",
        "    self.y = y\n",
        "    self.output = np.zeros(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_qhVKWOhmXCM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The output ŷ of a simple 2-layer Neural Network is:\n",
        "![The output ŷ of a simple 2-layer Neural Network is:](https://cdn-images-1.medium.com/max/800/1*E1_l8PGamc2xTNS87XGNcA.png/)"
      ]
    },
    {
      "metadata": {
        "id": "7nG3HKl5nH-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Naturally, the** right values for the weights and biases** determines the strength of the predictions. The process of fine-tuning the weights and biases from the input data is known as **training** the Neural Network."
      ]
    },
    {
      "metadata": {
        "id": "1RJUH-FxnTnn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- Training\n",
        "![Process](https://cdn-images-1.medium.com/max/800/1*CEtt0h8Rss_qPu7CyqMTdQ.png)"
      ]
    },
    {
      "metadata": {
        "id": "V-WBP2BPnONs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, x, y):\n",
        "    self.input = x\n",
        "    self.weights1 = np.random.rand(self.input.shape[1], 4)\n",
        "    self.weights2 = np.random.rand(4, 1)\n",
        "    self.y = y\n",
        "    self.output = np.zeros(y.shape)\n",
        "    \n",
        "  def feedforward(self):\n",
        "    self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
        "    self.output = sigmoid(np.dot(self.layer1, self.weights2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHuK3kykoq3P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loss Function\n",
        "\n",
        "To evaluate the goodness of our predicions\n",
        "\n",
        "![Loss Function](https://cdn-images-1.medium.com/max/800/1*iNa1VLdaeqwUAxpNXs3jwQ.png)"
      ]
    },
    {
      "metadata": {
        "id": "KK8CXb-8rIYw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Backpropagation\n",
        "\n",
        "In order to know the appropriate amount to adjust the weights and biases by, we need to know the derivative of the loss function with respect to the weights and biases.\n",
        "\n",
        "derivative of a function is simply the slope of the function.\n",
        "\n",
        "![Gradient descent algorithm](https://cdn-images-1.medium.com/max/800/1*3FgDOt4kJxK2QZlb9T0cpg.png)\n",
        "\n",
        "Gradient descent algorithm\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/800/1*7zxb2lfWWKaVxnmq2o69Mw.png)\n",
        "Chain rule for calculating derivative of the loss function with respect to the weights. Note that for simplicity, we have only displayed the partial derivative assuming a 1-layer Neural Network."
      ]
    },
    {
      "metadata": {
        "id": "O0eExsLwo4iI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "32dccc53-1256-4fe6-a10f-5486259d504c"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1.0/(1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  return x * (1.0 - x)\n",
        "  \n",
        "class NeuralNetwork:\n",
        "  def __init__(self, x, y):\n",
        "    self.input = x\n",
        "    self.weights1 = np.random.rand(self.input.shape[1], 4)\n",
        "    self.weights2 = np.random.rand(4, 1)\n",
        "    self.y = y\n",
        "    self.output = np.zeros(y.shape)\n",
        "    \n",
        "  def feedforward(self):\n",
        "    self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
        "    self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
        "    \n",
        "  def backprop(self):\n",
        "    # application of the chain rule to find derivative of the loss function with respect to to weights2 and weights1\n",
        "    d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
        "    d_weights1 = np.dot(self.input.T, (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
        "    \n",
        "    # update the weights with the derivative (slope) of the loss function\n",
        "    self.weights1 += d_weights1\n",
        "    self.weights2 += d_weights2\n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  X = np.array([[0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1]])\n",
        "  y = np.array([[0],[1],[1],[0]])\n",
        "  nn = NeuralNetwork(X,y)\n",
        "  \n",
        "  for i in range(1500):\n",
        "    nn.feedforward()\n",
        "    nn.backprop()\n",
        "    \n",
        "  print(nn.output)\n",
        "  "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.02206165]\n",
            " [0.97731644]\n",
            " [0.98142408]\n",
            " [0.02217002]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vrAlbXK4wTI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}